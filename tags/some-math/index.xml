<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Some Math on Trusted AI Ideas</title>
    <link>/tags/some-math/</link>
    <description>Recent content in Some Math on Trusted AI Ideas</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 16 May 2020 15:36:11 +0200</lastBuildDate>
    <atom:link href="/tags/some-math/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Do you know the 4 types of additive Variable Importances?</title>
      <link>/post/variable_importance_feature_attribution/</link>
      <pubDate>Sat, 16 May 2020 15:36:11 +0200</pubDate>
      <guid>/post/variable_importance_feature_attribution/</guid>
      <description>&lt;p&gt;Facing complex models, both computer simulation and machine learning practitioners have pursued similar objectives: to see how results could be broken down and linked to the inputs.&#xA;Whether it is called &lt;strong&gt;Sensitivity Analysis&lt;/strong&gt; or &lt;strong&gt;Variable Importance&lt;/strong&gt; in the context of explainable AI, some of their methods share an important component: the &lt;strong&gt;Shapley values&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;This article presents a structured 2 by 2 matrix to think about Variable Importances in terms of their goals.&#xA;Focused on additive feature attribution methods, the 4 identified quadrants are presented along with their &amp;ldquo;optimal&amp;rdquo; method: SHAP, SHAPLEY EFFECTS, SHAPloss and the very recent SAGE.&#xA;Then, we will look into Shapley values and their properties, which make the 4 methods theoretically optimal.&#xA;Finally, I will share my thoughts on the perspectives concerning Variable Importance methods.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
